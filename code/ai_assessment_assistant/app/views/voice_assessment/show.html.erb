<!-- Header with Logo -->
<header class="fixed top-0 left-0 right-0 z-40 w-full py-4 flex justify-center">
  <%= image_tag "Blue-Horizontal-LPL-Logo.svg", 
      alt: "LaunchPad Lab",
      class: "h-6 filter brightness-0 invert" %>
</header>

<!-- Gradient Background -->
<div class="min-h-screen bg-gradient-to-br from-blue-900 via-purple-900 to-indigo-900 flex items-center justify-center p-6 pt-24">
  
  <!-- Main Assessment Card -->
  <div class="backdrop-blur-md bg-white/10 rounded-3xl border border-white/20 shadow-2xl p-8 max-w-md w-full">
    
    <!-- Header -->
    <div class="text-center mb-8">
      <h1 class="text-2xl font-bold text-white mb-2">AI Assessment</h1>
      <h2 class="text-lg text-blue-200 mb-1"><%= @company.name %></h2>
      <p class="text-sm text-gray-400" id="welcomeTime"><%= Time.current.strftime("%l:%M %p") %></p>
    </div>
    
    <!-- Wave Visualizer -->
    <div class="mb-8">
      <!-- Loading State -->
      <div id="agentLoadingState" class="flex flex-col items-center justify-center h-16">
        <div class="flex items-center space-x-3">
          <div class="flex space-x-1">
            <div class="w-2 h-2 bg-cyan-400 rounded-full animate-bounce"></div>
            <div class="w-2 h-2 bg-cyan-400 rounded-full animate-bounce" style="animation-delay: 0.1s"></div>
            <div class="w-2 h-2 bg-cyan-400 rounded-full animate-bounce" style="animation-delay: 0.2s"></div>
          </div>
          <span class="text-cyan-300 text-sm font-medium">Agent is connecting...</span>
        </div>
      </div>
      
      <!-- Wave Animation (Hidden Initially) -->
      <div id="waveVisualizerContainer" class="hidden">
        <div class="flex items-end justify-center space-x-1 h-16">
          <% 10.times do |i| %>
            <div class="wave-bar bg-gradient-to-t from-cyan-500 to-blue-400 rounded-full transition-all duration-300" 
                 style="width: 4px; height: <%= rand(8..35) %>px;"></div>
          <% end %>
        </div>
      </div>
    </div>
    
    <!-- Transcript Area -->
    <div class="mb-8">
      <h3 class="text-white text-sm font-semibold mb-3 flex items-center">
        <svg class="w-4 h-4 mr-2 text-cyan-400" fill="currentColor" viewBox="0 0 24 24">
          <path d="M12 3v18l4-4h5a2 2 0 002-2V5a2 2 0 00-2-2H5a2 2 0 00-2 2v10a2 2 0 002 2h5l4 4z"/>
        </svg>
        Conversation
      </h3>
      
      <div id="transcriptContainer" class="bg-slate-900/50 rounded-2xl p-4 h-48 overflow-y-auto border border-slate-700/50">
        <div id="transcriptMessages" class="space-y-4">
          <!-- Messages will appear here -->
        </div>
        
        <!-- Typing Indicator -->
        <div id="typingIndicator" class="hidden flex items-center space-x-2 text-gray-400 text-sm mt-4">
          <div class="flex space-x-1">
            <div class="w-2 h-2 bg-cyan-400 rounded-full animate-bounce"></div>
            <div class="w-2 h-2 bg-cyan-400 rounded-full animate-bounce" style="animation-delay: 0.1s"></div>
            <div class="w-2 h-2 bg-cyan-400 rounded-full animate-bounce" style="animation-delay: 0.2s"></div>
          </div>
          <span>AI is responding...</span>
        </div>
      </div>
    </div>
    
    <!-- Control Bar -->
    <div class="flex items-center justify-center">
      
      <!-- Finish Assessment Button -->
      <button id="finishButton" class="px-6 py-3 bg-gradient-to-r from-red-500 to-red-600 hover:from-red-600 hover:to-red-700 text-white font-semibold rounded-xl shadow-lg border-2 border-red-400/30 hover:border-red-300/50 transition-all duration-300 transform hover:-translate-y-1 flex items-center space-x-2">
        <svg class="w-5 h-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
          <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 13l4 4L19 7"></path>
        </svg>
        <span>Finish Assessment</span>
      </button>
      
    </div>
    
    <!-- Helper Text -->
    <div class="text-center mt-6">
      <p class="text-gray-400 text-xs" id="statusText">
        Initializing AI assistant...
      </p>
    </div>
    
  </div>
  
</div>

<!-- Modern Confirmation Modal -->
<div id="confirmModal" class="fixed inset-0 z-50 hidden" aria-labelledby="modal-title" role="dialog" aria-modal="true">
  <div class="flex items-center justify-center min-h-screen p-4">
    <div class="fixed inset-0 bg-black/80 backdrop-blur-sm transition-opacity" aria-hidden="true"></div>
    
    <div class="relative backdrop-blur-md bg-white/10 rounded-2xl border border-white/20 shadow-2xl p-6 max-w-sm w-full">
      <div class="text-center">
        <div class="mx-auto mb-4 w-12 h-12 bg-gradient-to-r from-orange-400 to-red-500 rounded-full flex items-center justify-center">
          <svg class="w-6 h-6 text-white" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 9v2m0 4h.01m-6.938 4h13.856c1.54 0 2.502-1.667 1.732-2.5L13.732 4c-.77-.833-1.732-.833-2.5 0L4.268 15.5c-.77.833.192 2.5 1.732 2.5z"></path>
          </svg>
        </div>
        
        <h3 class="text-lg font-semibold text-white mb-2" id="modal-title">
          Finish Assessment?
        </h3>
        <p class="text-gray-300 text-sm mb-6">
          Are you sure you want to complete your assessment? This will save your responses and end the session.
        </p>
        
        <%= form_with url: complete_voice_assessment_path(@stakeholder.invitation_token), 
            method: :patch, 
            class: "mb-3",
            data: { turbo: false } do |form| %>
          <%= form.hidden_field :final_transcript, id: "finalTranscriptField" %>
          <%= form.submit "Yes, Finish Assessment", 
              class: "w-full px-4 py-3 bg-gradient-to-r from-cyan-500 to-blue-600 text-white font-medium rounded-xl hover:from-cyan-600 hover:to-blue-700 transition-all duration-200" %>
        <% end %>
        
        <button type="button" id="cancelButton" class="w-full px-4 py-3 bg-white/10 text-gray-200 font-medium rounded-xl hover:bg-white/20 transition-all duration-200 border border-white/20">
          Continue Assessment
        </button>
      </div>
    </div>
  </div>
</div>

<!-- Voice Assessment JavaScript -->
<script>
  document.addEventListener('DOMContentLoaded', function() {
    // Initialize voice assessment interface with proper WebRTC
    const VoiceAssessment = {
      init() {
        this.setupElements();
        this.setupEventListeners();
        this.initializeWaveVisualizer();
        this.setWelcomeTime();
        this.initializeVoiceSession();
      },

      setupElements() {
        this.finishButton = document.getElementById('finishButton');
        this.modal = document.getElementById('confirmModal');
        this.cancelButton = document.getElementById('cancelButton');
        this.statusText = document.getElementById('statusText');
        this.transcriptContainer = document.getElementById('transcriptContainer');
        this.transcriptMessages = document.getElementById('transcriptMessages');
        this.typingIndicator = document.getElementById('typingIndicator');
        this.finalTranscriptField = document.getElementById('finalTranscriptField');
        
        this.startTime = new Date();
        this.isConnected = false;
        this.transcriptData = [];
        this.conversationSession = null;
        
        // Real-time transcript state
        this.currentStreamingMessage = null;
        this.currentStreamingContent = '';
        this.isStreamingAI = false;
        this.isUserSpeaking = false;
        this.userSpeechIndicator = null;
        this.streamingThrottle = null;
        
        // WebRTC components
        this.peerConnection = null;
        this.dataChannel = null;
        this.audioElement = null;
        this.mediaStream = null;
        this.webrtcConfig = null;
        
        this.stakeholderToken = '<%= @stakeholder.invitation_token %>';
        this.openaiSession = <%= raw @openai_session.to_json %>;
        
        // Loading state elements
        this.agentLoadingState = document.getElementById('agentLoadingState');
        this.waveVisualizerContainer = document.getElementById('waveVisualizerContainer');
        
        // Flag to prevent multiple AI greetings
        this.hasGreeted = false;
        this.isInitializing = true;
        this.vadEnabled = false; // Track VAD state
      },

      setupEventListeners() {
        this.finishButton.addEventListener('click', () => this.showConfirmModal());
        this.cancelButton.addEventListener('click', () => this.hideConfirmModal());
        
        // Close modal on backdrop click
        this.modal.addEventListener('click', (e) => {
          if (e.target === this.modal) this.hideConfirmModal();
        });
      },

      async showConfirmModal() {
        // End voice session
        if (this.peerConnection) {
          await this.endVoiceSession();
        }
        
        // Save current transcript to hidden field
        this.finalTranscriptField.value = this.getTranscriptText();
        this.modal.classList.remove('hidden');
      },

      hideConfirmModal() {
        this.modal.classList.add('hidden');
      },

      addMessage(speaker, content, isUser = false) {
        const messageDiv = document.createElement('div');
        messageDiv.className = 'flex space-x-3';
        
        const avatarClass = isUser 
          ? 'w-8 h-8 bg-slate-600 rounded-full flex items-center justify-center'
          : 'w-8 h-8 bg-gradient-to-r from-blue-500 to-cyan-400 rounded-full flex items-center justify-center';
        
        const speakerLabel = isUser ? 'You' : 'Assistant';
        const speakerColor = isUser ? 'text-slate-300' : 'text-cyan-400';
        
        messageDiv.innerHTML = `
          <div class="flex-shrink-0">
            <div class="${avatarClass}">
              <span class="text-white text-xs font-semibold">${isUser ? 'You' : 'AI'}</span>
            </div>
          </div>
          <div class="flex-1">
            <p class="${speakerColor} text-sm font-medium mb-1">${speakerLabel}</p>
            <p class="text-slate-200 text-sm leading-relaxed">${content}</p>
            <span class="text-slate-500 text-xs mt-1 block">${this.getCurrentTime()}</span>
          </div>
        `;
        
        this.transcriptMessages.appendChild(messageDiv);
        this.scrollToBottom();
        
        // Save to transcript data and backend
        const transcriptEntry = {
          speaker: speakerLabel,
          content: content,
          timestamp: new Date().toISOString(),
          isUser: isUser
        };
        
        this.transcriptData.push(transcriptEntry);
        this.updateBackendTranscript(transcriptEntry);
      },

      async updateBackendTranscript(transcriptEntry) {
        if (!this.conversationSession) return;
        
        try {
          await fetch(`/api/voice/${this.stakeholderToken}/transcript`, {
            method: 'POST',
            headers: {
              'Content-Type': 'application/json',
              'X-CSRF-Token': document.querySelector('meta[name="csrf-token"]')?.getAttribute('content')
            },
            body: JSON.stringify({
              session_id: this.conversationSession.session_id,
              type: transcriptEntry.isUser ? 'user_speech' : 'ai_speech',
              content: transcriptEntry.content,
              speaker: transcriptEntry.isUser ? 'user' : 'assistant'
            })
          });
        } catch (error) {
          console.error('Error updating transcript:', error);
        }
      },

      scrollToBottom() {
        this.transcriptContainer.scrollTop = this.transcriptContainer.scrollHeight;
      },

      getCurrentTime() {
        return new Date().toLocaleTimeString([], { hour: '2-digit', minute: '2-digit' });
      },

      setWelcomeTime() {
        // Welcome time is now set by Rails template - no JavaScript needed
      },

      getTranscriptText() {
        return this.transcriptData.map(msg => 
          `[${msg.timestamp}] ${msg.speaker}: ${msg.content}`
        ).join('\n');
      },

      initializeWaveVisualizer() {
        const waveBars = document.querySelectorAll('.wave-bar');
        
        const randomizeWaves = () => {
          waveBars.forEach((bar, index) => {
            setTimeout(() => {
              const height = Math.random() * 35 + 8;
              bar.style.height = height + 'px';
            }, index * 50);
          });
        };
        
        // Don't start the animation until agent is ready
        // this.waveInterval = setInterval(randomizeWaves, 1500);
      },
      
      showAgentReady() {
        // Transition from loading to wave animation
        if (this.agentLoadingState && this.waveVisualizerContainer) {
          this.agentLoadingState.classList.add('hidden');
          this.waveVisualizerContainer.classList.remove('hidden');
          
          // Start the wave animation
          const waveBars = document.querySelectorAll('.wave-bar');
          const randomizeWaves = () => {
            waveBars.forEach((bar, index) => {
              setTimeout(() => {
                const height = Math.random() * 35 + 8;
                bar.style.height = height + 'px';
              }, index * 50);
            });
          };
          
          // Start the animation
          this.waveInterval = setInterval(randomizeWaves, 1500);
          randomizeWaves(); // Initial animation
        }
      },

      updateWaveVisualizerWithAudio(audioLevel) {
        const waveBars = document.querySelectorAll('.wave-bar');
        waveBars.forEach((bar, index) => {
          const intensity = audioLevel * (1 - Math.abs(index - 5) / 5);
          const height = Math.max(8, intensity * 40);
          bar.style.height = height + 'px';
        });
      },

      // Initialize voice conversation session
      async initializeVoiceSession() {
        try {
          // Use session data already created on server
          this.conversationSession = this.openaiSession;
          console.log('🎯 Voice session initialized:', this.conversationSession.session_id);
          console.log('🏢 Company context:', this.conversationSession.company_name);
          console.log('👤 Stakeholder:', this.conversationSession.stakeholder_name);
          
          // Get WebRTC configuration with ephemeral token
          await this.getWebRTCConfig();
          
          // Auto-start the voice conversation
          await this.startVoiceConversation();
          
        } catch (error) {
          console.error('Error initializing voice session:', error);
          this.updateConnectionStatus('error', 'Connection failed. Please refresh and try again.');
          this.addMessage('System', 'Voice session initialization failed. Please refresh the page and try again.', false);
        }
      },

      updateConnectionStatus(state, message) {
        // Update status text only since visual indicators were removed
        switch (state) {
          case 'connecting':
            this.statusText.textContent = message || 'Connecting to AI assistant...';
            break;
          case 'connected':
            this.statusText.textContent = message || 'Connected - Conversation in progress';
            break;
          case 'error':
            this.statusText.textContent = message || 'Connection error';
            this.statusText.className = 'text-red-400 text-xs';
            break;
        }
      },

      async getWebRTCConfig() {
        try {
          const response = await fetch(`/api/voice/${this.stakeholderToken}/realtime_config?session_id=${this.conversationSession.session_id}`, {
            method: 'GET',
            headers: {
              'Content-Type': 'application/json'
            }
          });
          
          const data = await response.json();
          if (data.success) {
            this.webrtcConfig = data.config;
            console.log('🔧 WebRTC config loaded with ephemeral token');
            console.log('⏰ Token expires at:', this.webrtcConfig.session_expires_at);
          } else {
            throw new Error(data.error || 'Failed to get WebRTC config');
          }
        } catch (error) {
          console.error('Error getting WebRTC config:', error);
          throw error;
        }
      },

      async startVoiceConversation() {
        try {
          console.log('🎤 Starting WebRTC voice conversation...');
          this.updateConnectionStatus('connecting', 'Requesting microphone access...');
          
          if (!this.webrtcConfig) {
            await this.getWebRTCConfig();
          }
          
          // Check if ephemeral token is still valid
          const expiresAt = new Date(this.webrtcConfig.session_expires_at);
          if (expiresAt < new Date()) {
            console.log('🔄 Token expired, refreshing...');
            await this.getWebRTCConfig();
          }
          
          // Create RTCPeerConnection
          this.peerConnection = new RTCPeerConnection({
            iceServers: [{ urls: 'stun:stun.l.google.com:19302' }]
          });
          
          // Set up audio element for AI responses
          this.audioElement = document.createElement('audio');
          this.audioElement.autoplay = true;
          
          // Handle remote audio stream from AI
          this.peerConnection.ontrack = (event) => {
            console.log('🔊 Received AI audio stream');
            this.audioElement.srcObject = event.streams[0];
          };
          
          // Request microphone permission with optimized settings
          this.mediaStream = await navigator.mediaDevices.getUserMedia({ 
            audio: {
              sampleRate: 24000,
              channelCount: 1,
              echoCancellation: true,
              noiseSuppression: true,
              autoGainControl: true,
              sampleSize: 16,
              // Additional constraints to reduce feedback
              googEchoCancellation: true,
              googAutoGainControl: true,
              googNoiseSuppression: true,
              googHighpassFilter: true
            } 
          });
          
          this.updateConnectionStatus('connecting', 'Connecting to AI assistant...');
          
          // Add local audio track to peer connection
          this.mediaStream.getTracks().forEach(track => {
            this.peerConnection.addTrack(track, this.mediaStream);
          });
          
          // Create data channel for sending/receiving events
          this.dataChannel = this.peerConnection.createDataChannel('oai-events');
          
          // Handle incoming events from OpenAI
          this.dataChannel.addEventListener('message', (event) => {
            this.handleOpenAIEvent(JSON.parse(event.data));
          });
          
          this.dataChannel.addEventListener('open', () => {
            console.log('✅ Data channel opened - ready for conversation!');
            
            // Send session configuration with VAD disabled initially
            this.sendOpenAIEvent({
              type: 'session.update',
              session: {
                modalities: ['text', 'audio'],
                instructions: this.webrtcConfig.instructions,
                voice: this.webrtcConfig.voice,
                input_audio_format: 'pcm16',
                output_audio_format: 'pcm16',
                input_audio_transcription: { model: 'whisper-1' },
                turn_detection: null, // Disabled initially to prevent phantom responses
                temperature: 0.7
              }
            });
            
            // Trigger AI to start the conversation only once
            if (!this.hasGreeted && this.isInitializing) {
              this.hasGreeted = true;  // Set immediately to prevent multiple calls
              setTimeout(() => {
                console.log('🚀 Triggering AI to start conversation...');
                
                // Show agent is ready
                this.showAgentReady();
                this.updateConnectionStatus('connected', 'Agent is ready - starting conversation...');
                
                this.sendOpenAIEvent({
                  type: 'response.create'
                });
                // Keep isInitializing = true until VAD is enabled
              }, 1500); // 1.5 seconds - balance between stability and UX
            }
          });
          
          // Create SDP offer
          const offer = await this.peerConnection.createOffer();
          await this.peerConnection.setLocalDescription(offer);
          
          // Send SDP offer to OpenAI Realtime API
          const response = await fetch(`${this.webrtcConfig.api_endpoint}?model=${this.webrtcConfig.model}`, {
            method: 'POST',
            headers: {
              'Authorization': `Bearer ${this.webrtcConfig.ephemeral_token}`,
              'Content-Type': 'application/sdp'
            },
            body: offer.sdp
          });
          
          if (!response.ok) {
            throw new Error(`SDP exchange failed: ${response.status} ${response.statusText}`);
          }
          
          // Get SDP answer and set as remote description
          const answerSdp = await response.text();
          const answer = {
            type: 'answer',
            sdp: answerSdp
          };
          
          await this.peerConnection.setRemoteDescription(answer);
          
          // Update UI to connected state
          this.isConnected = true;
          this.updateConnectionStatus('connected', 'Connected - preparing to start...');
          
          console.log('✅ WebRTC voice conversation started successfully!');
          
        } catch (error) {
          console.error('Error starting WebRTC voice conversation:', error);
          this.updateConnectionStatus('error', `Failed to start conversation: ${error.message}`);
          this.addMessage('System', `Failed to start voice conversation: ${error.message}. Please check microphone permissions and try again.`, false);
        }
      },

      sendOpenAIEvent(event) {
        if (this.dataChannel && this.dataChannel.readyState === 'open') {
          this.dataChannel.send(JSON.stringify(event));
        } else {
          console.warn('Data channel not ready for sending events');
        }
      },

      enableVoiceActivation() {
        console.log('🎤 Enabling Voice Activity Detection...');
        this.vadEnabled = true;
        this.sendOpenAIEvent({
          type: 'session.update',
          session: {
            turn_detection: {
              type: 'server_vad',
              threshold: 0.7,  // Slightly more sensitive
              prefix_padding_ms: 300,  // Reduced padding for faster response
              silence_duration_ms: 800  // Much faster response - 0.8 seconds instead of 2
            }
          }
        });
      },

      handleOpenAIEvent(event) {
        console.log('📨 OpenAI event:', event.type, event);
        
        switch (event.type) {
          case 'session.created':
            console.log('✅ Session created with ID:', event.session?.id);
            break;
            
          case 'session.updated':
            console.log('✅ Session updated');
            break;
            
          case 'conversation.item.created':
            console.log('💬 Conversation item created:', event.item?.role);
            break;
            
          case 'input_audio_buffer.speech_started':
            // User started speaking
            this.showUserSpeechIndicator();
            break;
            
          case 'input_audio_buffer.speech_stopped':
            // User stopped speaking
            this.hideUserSpeechIndicator();
            break;
            
          case 'conversation.item.input_audio_transcription.completed':
            // Only process user speech if VAD is enabled (prevents phantom responses during init)
            if (this.vadEnabled && event.transcript && event.transcript.trim().length > 2) {
              // Skip common noise transcriptions
              const noisePatterns = /^(um|uh|hmm|mm|ah|oh|bye|hi|hello|hey|thank you|thanks|yes|no|ok|okay)$/i;
              if (!noisePatterns.test(event.transcript.trim()) || event.transcript.trim().length > 5) {
                this.hideUserSpeechIndicator(); // Hide indicator before showing message
                this.addMessage('You', event.transcript, true);
              }
            } else if (!this.vadEnabled) {
              console.log('🚫 Ignoring user speech during initialization:', event.transcript);
            }
            break;
            
          case 'response.created':
            console.log('🎤 Response created - AI should start speaking');
            this.startStreamingAIMessage();
            break;
            
          case 'response.audio_transcript.delta':
            // AI speech being generated (real-time)
            this.handleAIResponseDelta(event.delta);
            break;
            
          case 'response.audio_transcript.done':
            // AI response complete
            console.log('📝 AI transcript complete:', event.transcript);
            this.finalizeStreamingAIMessage(event.transcript);
            break;
            
          case 'response.audio.delta':
            // Raw audio data being received
            console.log('🔊 Receiving audio data...');
            break;
            
          case 'response.audio.done':
            console.log('🔊 Audio playback complete');
            break;
            
          case 'response.done':
            console.log('🎯 AI response completed');
            console.log('🔍 Debug - isInitializing:', this.isInitializing, 'vadEnabled:', this.vadEnabled);
            
            // Enable VAD after the first AI greeting is complete
            if (this.isInitializing && !this.vadEnabled) {
              console.log('🎉 Initial greeting complete - enabling voice input');
              this.enableVoiceActivation();
              this.isInitializing = false;
            } else {
              console.log('⚠️ VAD not enabled - condition not met');
            }
            break;
            
          case 'error':
            console.error('❌ OpenAI error:', event.error);
            this.addMessage('System', `Error: ${event.error?.message || JSON.stringify(event.error)}`, false);
            break;
            
          default:
            console.log('📋 Unhandled event type:', event.type);
        }
      },

      handleAIResponseDelta(delta) {
        // Handle real-time AI response text with throttling for smoother appearance
        if (delta && this.isStreamingAI && this.currentStreamingMessage) {
          console.log('🔤 AI speaking:', delta);
          
          // Append the delta to current content
          this.currentStreamingContent += delta;
          
          // Throttle updates to make text appear more smoothly (every ~100ms)
          if (!this.streamingThrottle) {
            this.streamingThrottle = setTimeout(() => {
              const contentElement = this.currentStreamingMessage.querySelector('.streaming-content');
              if (contentElement) {
                contentElement.textContent = this.currentStreamingContent;
                this.scrollToBottom();
              }
              this.streamingThrottle = null;
            }, 100);
          }
        }
      },
      
      startStreamingAIMessage() {
        if (this.isStreamingAI) return; // Already streaming
        
        console.log('🎬 Starting streaming AI message...');
        this.isStreamingAI = true;
        this.currentStreamingContent = '';
        
        // Create streaming message container
        const messageDiv = document.createElement('div');
        messageDiv.className = 'flex space-x-3';
        
        messageDiv.innerHTML = `
          <div class="flex-shrink-0">
            <div class="w-8 h-8 bg-gradient-to-r from-blue-500 to-cyan-400 rounded-full flex items-center justify-center">
              <span class="text-white text-xs font-semibold">AI</span>
            </div>
          </div>
          <div class="flex-1">
            <p class="text-cyan-400 text-sm font-medium mb-1">Assistant</p>
            <p class="text-slate-200 text-sm leading-relaxed streaming-content"></p>
            <span class="text-slate-500 text-xs mt-1 block">${this.getCurrentTime()}</span>
          </div>
        `;
        
        this.currentStreamingMessage = messageDiv;
        this.transcriptMessages.appendChild(messageDiv);
        this.scrollToBottom();
      },
      
      finalizeStreamingAIMessage(finalTranscript) {
        if (!this.isStreamingAI || !this.currentStreamingMessage) return;
        
        console.log('🏁 Finalizing streaming AI message:', finalTranscript);
        
        // Use final transcript if provided, otherwise use accumulated content
        const finalContent = finalTranscript || this.currentStreamingContent;
        
        if (finalContent && finalContent.trim()) {
          // Update final content
          const contentElement = this.currentStreamingMessage.querySelector('.streaming-content');
          if (contentElement) {
            contentElement.textContent = finalContent;
          }
          
          // Save to transcript data and backend
          const transcriptEntry = {
            speaker: 'Assistant',
            content: finalContent,
            timestamp: new Date().toISOString(),
            isUser: false
          };
          
          this.transcriptData.push(transcriptEntry);
          this.updateBackendTranscript(transcriptEntry);
        }
        
        // Reset streaming state
        this.isStreamingAI = false;
        this.currentStreamingMessage = null;
        this.currentStreamingContent = '';
        
        // Clear any pending throttle
        if (this.streamingThrottle) {
          clearTimeout(this.streamingThrottle);
          this.streamingThrottle = null;
        }
        
        this.scrollToBottom();
      },
      
      showUserSpeechIndicator() {
        if (this.isUserSpeaking || this.userSpeechIndicator) return;
        
        console.log('🎤 User listening...');
        this.isUserSpeaking = true;
        
        // Create user listening indicator with animated dots
        const indicatorDiv = document.createElement('div');
        indicatorDiv.className = 'flex space-x-3 user-speech-indicator';
        
        indicatorDiv.innerHTML = `
          <div class="flex-shrink-0">
            <div class="w-8 h-8 bg-slate-600 rounded-full flex items-center justify-center">
              <span class="text-white text-xs font-semibold">You</span>
            </div>
          </div>
          <div class="flex-1">
            <p class="text-slate-300 text-sm font-medium mb-1">You</p>
            <div class="flex items-center space-x-2 text-slate-400 text-sm">
              <div class="flex space-x-1.5">
                <div class="w-1.5 h-1.5 bg-slate-400 rounded-full animate-pulse"></div>
                <div class="w-1.5 h-1.5 bg-slate-400 rounded-full animate-pulse" style="animation-delay: 0.2s"></div>
                <div class="w-1.5 h-1.5 bg-slate-400 rounded-full animate-pulse" style="animation-delay: 0.4s"></div>
              </div>
            </div>
            <span class="text-slate-500 text-xs mt-1 block">${this.getCurrentTime()}</span>
          </div>
        `;
        
        this.userSpeechIndicator = indicatorDiv;
        this.transcriptMessages.appendChild(indicatorDiv);
        this.scrollToBottom();
      },
      
      hideUserSpeechIndicator() {
        if (!this.isUserSpeaking || !this.userSpeechIndicator) return;
        
        console.log('🔇 User finished speaking');
        this.isUserSpeaking = false;
        
        // Remove the indicator
        if (this.userSpeechIndicator.parentNode) {
          this.userSpeechIndicator.parentNode.removeChild(this.userSpeechIndicator);
        }
        
        this.userSpeechIndicator = null;
      },

      async stopVoiceConversation() {
        console.log('🛑 Stopping WebRTC voice conversation...');
        
        // Close data channel
        if (this.dataChannel) {
          this.dataChannel.close();
          this.dataChannel = null;
        }
        
        // Close peer connection
        if (this.peerConnection) {
          this.peerConnection.close();
          this.peerConnection = null;
        }
        
        // Stop media stream
        if (this.mediaStream) {
          this.mediaStream.getTracks().forEach(track => track.stop());
          this.mediaStream = null;
        }
        
        // Remove audio element
        if (this.audioElement) {
          this.audioElement.pause();
          this.audioElement.srcObject = null;
          this.audioElement = null;
        }
        
        // Update UI to disconnected state
        this.isConnected = false;
        this.updateConnectionStatus('connecting', 'Conversation ended');
        
        console.log('✅ WebRTC voice conversation stopped');
      },

      // End voice session
      async endVoiceSession() {
        // Clean up any streaming states
        this.hideUserSpeechIndicator();
        if (this.isStreamingAI) {
          this.finalizeStreamingAIMessage(this.currentStreamingContent);
        }
        
        // Clear any pending throttle
        if (this.streamingThrottle) {
          clearTimeout(this.streamingThrottle);
          this.streamingThrottle = null;
        }
        
        await this.stopVoiceConversation();
        
        try {
          const response = await fetch(`/api/voice/${this.stakeholderToken}/end`, {
            method: 'POST',
            headers: {
              'Content-Type': 'application/json',
              'X-CSRF-Token': document.querySelector('meta[name="csrf-token"]')?.getAttribute('content')
            },
            body: JSON.stringify({
              session_id: this.conversationSession.session_id
            })
          });
          
          const data = await response.json();
          if (data.success) {
            console.log('🏁 Voice session ended');
            if (data.final_message) {
              this.addMessage('Assistant', data.final_message, false);
            }
          }
        } catch (error) {
          console.error('Error ending voice session:', error);
        }
      },

      // Send text message (for testing/fallback)
      async sendTextMessage(message) {
        if (!this.conversationSession) {
          console.error('No active conversation session');
          return;
        }
        
        if (!this.vadEnabled) {
          console.warn('Voice input not yet enabled - message may be processed after initialization');
        }
        
        try {
          // Add user message to transcript
          this.addMessage('You', message, true);
          
          const response = await fetch(`/api/voice/${this.stakeholderToken}/message`, {
            method: 'POST',
            headers: {
              'Content-Type': 'application/json',
              'X-CSRF-Token': document.querySelector('meta[name="csrf-token"]')?.getAttribute('content')
            },
            body: JSON.stringify({
              message: message,
              session_id: this.conversationSession.session_id
            })
          });
          
          const data = await response.json();
          if (data.success) {
            console.log('📝 Text message processed');
          }
        } catch (error) {
          console.error('Error sending message:', error);
        }
      }
    };

    // Initialize the voice assessment interface
    VoiceAssessment.init();
    
    // Make it globally available for testing
    window.VoiceAssessment = VoiceAssessment;
  });
</script>

<!-- Header with Logo -->
<header class="fixed top-0 left-0 right-0 z-40 p-6">
  <div class="flex justify-center">
    <div class="w-40 h-10 bg-gradient-to-r from-lpl-blue to-lpl-cyan rounded-lg flex items-center justify-center shadow-lg">
      <%= image_tag "White-Horizontal-LPL-Logo.svg", 
          alt: "LaunchPad Lab", 
          class: "h-6" %>
    </div>
  </div>
</header>

<!-- Gradient Background -->
<div class="min-h-screen bg-gradient-to-br from-blue-900 via-purple-900 to-indigo-900 flex items-center justify-center p-6 pt-24">
  
  <!-- Main Assessment Card -->
  <div class="backdrop-blur-md bg-white/10 rounded-3xl border border-white/20 shadow-2xl p-8 max-w-md w-full">
    
    <!-- Header -->
    <div class="text-center mb-8">
      <h1 class="text-2xl font-bold text-white mb-2">AI Assessment</h1>
      <p class="text-blue-200 text-sm"><%= @company.name %></p>
    </div>
    
    <!-- Futuristic Wave Visualization -->
    <div class="mb-8 bg-white/5 rounded-2xl p-6 border border-white/10">
      <div id="waveVisualizer" class="flex items-center justify-center space-x-2 h-16">
        <% 10.times do |i| %>
          <div class="wave-bar w-1.5 rounded-full bg-gradient-to-t from-cyan-500 to-blue-600" style="animation-delay: <%= i * 0.1 %>s; height: 8px;"></div>
        <% end %>
      </div>
    </div>
    
    <!-- Conversation Transcript -->
    <div id="transcriptContainer" class="mb-8 bg-white/5 rounded-2xl p-4 max-h-64 overflow-y-auto transcript-scroll border border-white/10">
      <div id="transcriptMessages" class="space-y-4">
        
        <!-- Initial AI Welcome Message -->
        <div class="flex space-x-3">
          <div class="flex-shrink-0">
            <div class="w-8 h-8 bg-gradient-to-r from-cyan-500 to-blue-600 rounded-full flex items-center justify-center">
              <span class="text-white text-xs font-semibold">AI</span>
            </div>
          </div>
          <div class="flex-1">
            <p class="text-cyan-300 text-sm font-medium mb-1">Assistant</p>
            <p class="text-gray-100 text-sm leading-relaxed" id="welcomeMessage">
              Hello! I'm here to learn about <%= @company.name %>'s AI readiness. Could you start by telling me about your role and responsibilities?
            </p>
            <span class="text-gray-400 text-xs mt-1 block" id="welcomeTime"></span>
          </div>
        </div>

        <!-- Typing indicator (initially hidden) -->
        <div id="typingIndicator" class="hidden flex space-x-3">
          <div class="flex-shrink-0">
            <div class="w-8 h-8 bg-gradient-to-r from-cyan-500 to-blue-600 rounded-full flex items-center justify-center">
              <span class="text-white text-xs font-semibold">AI</span>
            </div>
          </div>
          <div class="flex-1">
            <p class="text-cyan-300 text-sm font-medium mb-1">Assistant</p>
            <div class="flex space-x-1">
              <div class="w-2 h-2 bg-cyan-400 rounded-full animate-bounce"></div>
              <div class="w-2 h-2 bg-cyan-400 rounded-full animate-bounce" style="animation-delay: 0.1s;"></div>
              <div class="w-2 h-2 bg-cyan-400 rounded-full animate-bounce" style="animation-delay: 0.2s;"></div>
            </div>
          </div>
        </div>
      </div>
    </div>
    
    <!-- Duration Status -->
    <div class="text-center mb-6">
      <p class="text-gray-300 text-sm">
        Duration: <span id="durationTimer" class="text-cyan-300 font-medium"><%= @assessment_duration %>:00</span>
      </p>
    </div>
    
    <!-- Control Bar -->
    <div class="flex items-center justify-center space-x-6">
      
      <!-- Settings Button -->
      <button id="settingsButton" class="control-button w-12 h-12 bg-white/10 rounded-full flex items-center justify-center border border-white/20 hover:bg-white/20 transition-all duration-300">
        <svg class="w-5 h-5 text-gray-300" fill="none" stroke="currentColor" viewBox="0 0 24 24">
          <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10.325 4.317c.426-1.756 2.924-1.756 3.35 0a1.724 1.724 0 002.573 1.066c1.543-.94 3.31.826 2.37 2.37a1.724 1.724 0 001.065 2.572c1.756.426 1.756 2.924 0 3.35a1.724 1.724 0 00-1.066 2.573c.94 1.543-.826 3.31-2.37 2.37a1.724 1.724 0 00-2.572 1.065c-.426 1.756-2.924 1.756-3.35 0a1.724 1.724 0 00-2.573-1.066c-1.543.94-3.31-.826-2.37-2.37a1.724 1.724 0 00-1.065-2.572c-1.756-.426-1.756-2.924 0-3.35a1.724 1.724 0 001.066-2.573c-.94-1.543.826-3.31 2.37-2.37.996.608 2.296.07 2.572-1.065z"></path>
          <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M15 12a3 3 0 11-6 0 3 3 0 016 0z"></path>
        </svg>
      </button>
      
      <!-- Main Microphone Button -->
      <button id="microphoneButton" class="control-button w-16 h-16 bg-gradient-to-r from-cyan-500 to-blue-600 rounded-full flex items-center justify-center shadow-lg border-2 border-white/20 hover:from-cyan-600 hover:to-blue-700 transition-all duration-300 transform hover:-translate-y-1">
        <svg class="w-8 h-8 text-white" fill="currentColor" viewBox="0 0 24 24">
          <path d="M12 14c1.66 0 2.99-1.34 2.99-3L15 5c0-1.66-1.34-3-3-3S9 3.34 9 5v6c0 1.66 1.34 3 3 3zm5.3-3c0 3-2.54 5.1-5.3 5.1S6.7 14 6.7 11H5c0 3.41 2.72 6.23 6 6.72V21h2v-3.28c3.28-.48 6-3.3 6-6.72h-1.7z"/>
        </svg>
      </button>
      
      <!-- End Assessment Button -->
      <button id="finishButton" class="control-button w-12 h-12 bg-red-500/20 rounded-full flex items-center justify-center border border-red-400/30 hover:bg-red-500/30 transition-all duration-300">
        <svg class="w-5 h-5 text-red-400" fill="none" stroke="currentColor" viewBox="0 0 24 24">
          <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M6 18L18 6M6 6l12 12"></path>
        </svg>
      </button>
      
    </div>
    
    <!-- Helper Text -->
    <div class="text-center mt-6">
      <p class="text-gray-400 text-xs">
        Tap the microphone to continue ‚Ä¢ Tap X to finish
      </p>
    </div>
    
  </div>
  
</div>

<!-- Modern Confirmation Modal -->
<div id="confirmModal" class="fixed inset-0 z-50 hidden" aria-labelledby="modal-title" role="dialog" aria-modal="true">
  <div class="flex items-center justify-center min-h-screen p-4">
    <div class="fixed inset-0 bg-black/80 backdrop-blur-sm transition-opacity" aria-hidden="true"></div>
    
    <div class="relative backdrop-blur-md bg-white/10 rounded-2xl border border-white/20 shadow-2xl p-6 max-w-sm w-full">
      <div class="text-center">
        <div class="mx-auto mb-4 w-12 h-12 bg-gradient-to-r from-orange-400 to-red-500 rounded-full flex items-center justify-center">
          <svg class="w-6 h-6 text-white" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 9v2m0 4h.01m-6.938 4h13.856c1.54 0 2.502-1.667 1.732-2.5L13.732 4c-.77-.833-1.732-.833-2.5 0L4.268 15.5c-.77.833.192 2.5 1.732 2.5z"></path>
          </svg>
        </div>
        
        <h3 class="text-lg font-semibold text-white mb-2" id="modal-title">
          Finish Assessment?
        </h3>
        <p class="text-gray-300 text-sm mb-6">
          Are you sure you want to complete your assessment? This will save your responses and end the session.
        </p>
        
        <%= form_with url: complete_voice_assessment_path(@stakeholder.invitation_token), 
            method: :patch, 
            class: "space-y-3",
            data: { turbo: false } do |form| %>
          <%= form.hidden_field :final_transcript, id: "finalTranscriptField" %>
          <%= form.submit "Yes, Finish Assessment", 
              class: "w-full px-4 py-3 bg-gradient-to-r from-cyan-500 to-blue-600 text-white font-medium rounded-xl hover:from-cyan-600 hover:to-blue-700 transition-all duration-200" %>
        <% end %>
        
        <button type="button" id="cancelButton" class="w-full px-4 py-3 bg-white/10 text-gray-200 font-medium rounded-xl hover:bg-white/20 transition-all duration-200 border border-white/20">
          Continue Assessment
        </button>
      </div>
    </div>
  </div>
</div>

<!-- Voice Assessment JavaScript -->
<script>
  document.addEventListener('DOMContentLoaded', function() {
    // Initialize voice assessment interface with proper WebRTC
    const VoiceAssessment = {
      init() {
        this.setupElements();
        this.setupEventListeners();
        this.startDurationTimer();
        this.initializeWaveVisualizer();
        this.setWelcomeTime();
        this.initializeVoiceSession();
      },

      setupElements() {
        this.finishButton = document.getElementById('finishButton');
        this.modal = document.getElementById('confirmModal');
        this.cancelButton = document.getElementById('cancelButton');
        this.microphoneButton = document.getElementById('microphoneButton');
        this.settingsButton = document.getElementById('settingsButton');
        this.transcriptContainer = document.getElementById('transcriptContainer');
        this.transcriptMessages = document.getElementById('transcriptMessages');
        this.typingIndicator = document.getElementById('typingIndicator');
        this.durationTimer = document.getElementById('durationTimer');
        this.finalTranscriptField = document.getElementById('finalTranscriptField');
        
        this.startTime = new Date();
        this.isConnected = false;
        this.transcriptData = [];
        this.conversationSession = null;
        
        // WebRTC components
        this.peerConnection = null;
        this.dataChannel = null;
        this.audioElement = null;
        this.mediaStream = null;
        this.webrtcConfig = null;
        
        this.stakeholderToken = '<%= @stakeholder.invitation_token %>';
        this.openaiSession = <%= raw @openai_session.to_json %>;
      },

      setupEventListeners() {
        this.finishButton.addEventListener('click', () => this.showConfirmModal());
        this.cancelButton.addEventListener('click', () => this.hideConfirmModal());
        this.microphoneButton.addEventListener('click', () => this.toggleVoiceConversation());
        this.settingsButton.addEventListener('click', () => this.showSettings());
        this.settingsButton.addEventListener('dblclick', () => this.testTextInput());
        
        // Close modal on backdrop click
        this.modal.addEventListener('click', (e) => {
          if (e.target === this.modal) this.hideConfirmModal();
        });
      },

      async showConfirmModal() {
        // End voice session
        if (this.peerConnection) {
          await this.endVoiceSession();
        }
        
        // Save current transcript to hidden field
        this.finalTranscriptField.value = this.getTranscriptText();
        this.modal.classList.remove('hidden');
      },

      hideConfirmModal() {
        this.modal.classList.add('hidden');
      },

      // Test method for text input (fallback)
      async testTextInput() {
        const testMessage = "I work as a software engineer and am interested in how AI can help with code reviews and automated testing.";
        await this.sendTextMessage(testMessage);
      },

      showSettings() {
        alert('Settings: Voice conversation powered by OpenAI Realtime API via WebRTC\n\nFor best results:\n‚Ä¢ Use Chrome, Edge, or Safari\n‚Ä¢ Ensure stable internet connection\n‚Ä¢ Speak clearly at normal volume\n‚Ä¢ Wait for AI to finish speaking before responding\n\nNote: Ephemeral tokens expire every minute for security');
      },

      addMessage(speaker, content, isUser = false) {
        const messageDiv = document.createElement('div');
        messageDiv.className = 'flex space-x-3';
        
        const avatarClass = isUser 
          ? 'w-8 h-8 bg-slate-600 rounded-full flex items-center justify-center'
          : 'w-8 h-8 bg-gradient-to-r from-lpl-blue to-lpl-cyan rounded-full flex items-center justify-center';
        
        const speakerLabel = isUser ? 'You' : 'Assistant';
        const speakerColor = isUser ? 'text-slate-300' : 'text-lpl-cyan';
        
        messageDiv.innerHTML = `
          <div class="flex-shrink-0">
            <div class="${avatarClass}">
              <span class="text-white text-xs font-semibold">${isUser ? 'You' : 'AI'}</span>
            </div>
          </div>
          <div class="flex-1">
            <p class="${speakerColor} text-sm font-medium mb-1">${speakerLabel}</p>
            <p class="text-slate-200 text-sm leading-relaxed">${content}</p>
            <span class="text-slate-500 text-xs mt-1 block">${this.getCurrentTime()}</span>
          </div>
        `;
        
        this.transcriptMessages.appendChild(messageDiv);
        this.scrollToBottom();
        
        // Save to transcript data and backend
        const transcriptEntry = {
          speaker: speakerLabel,
          content: content,
          timestamp: new Date().toISOString(),
          isUser: isUser
        };
        
        this.transcriptData.push(transcriptEntry);
        this.updateBackendTranscript(transcriptEntry);
      },

      async updateBackendTranscript(transcriptEntry) {
        if (!this.conversationSession) return;
        
        try {
          await fetch(`/api/voice/${this.stakeholderToken}/transcript`, {
            method: 'POST',
            headers: {
              'Content-Type': 'application/json',
              'X-CSRF-Token': document.querySelector('meta[name="csrf-token"]')?.getAttribute('content')
            },
            body: JSON.stringify({
              session_id: this.conversationSession.session_id,
              type: transcriptEntry.isUser ? 'user_speech' : 'ai_speech',
              content: transcriptEntry.content,
              speaker: transcriptEntry.isUser ? 'user' : 'assistant'
            })
          });
        } catch (error) {
          console.error('Error updating transcript:', error);
        }
      },

      scrollToBottom() {
        this.transcriptContainer.scrollTop = this.transcriptContainer.scrollHeight;
      },

      getCurrentTime() {
        return new Date().toLocaleTimeString([], { hour: '2-digit', minute: '2-digit' });
      },

      setWelcomeTime() {
        const welcomeTime = document.getElementById('welcomeTime');
        if (welcomeTime) {
          welcomeTime.textContent = this.getCurrentTime();
        }
      },

      getTranscriptText() {
        return this.transcriptData.map(msg => 
          `[${msg.timestamp}] ${msg.speaker}: ${msg.content}`
        ).join('\n');
      },

      startDurationTimer() {
        setInterval(() => {
          const elapsed = Math.floor((new Date() - this.startTime) / 1000);
          const minutes = Math.floor(elapsed / 60);
          const seconds = elapsed % 60;
          this.durationTimer.textContent = `${minutes}:${seconds.toString().padStart(2, '0')}`;
        }, 1000);
      },

      initializeWaveVisualizer() {
        const waveBars = document.querySelectorAll('.wave-bar');
        
        const randomizeWaves = () => {
          waveBars.forEach((bar, index) => {
            setTimeout(() => {
              const height = Math.random() * 35 + 8;
              bar.style.height = height + 'px';
            }, index * 50);
          });
        };
        
        // Update waves every 1.5 seconds for dynamic effect
        this.waveInterval = setInterval(randomizeWaves, 1500);
      },

      updateWaveVisualizerWithAudio(audioLevel) {
        const waveBars = document.querySelectorAll('.wave-bar');
        waveBars.forEach((bar, index) => {
          const intensity = audioLevel * (1 - Math.abs(index - 5) / 5);
          const height = Math.max(8, intensity * 40);
          bar.style.height = height + 'px';
        });
      },

      // Initialize voice conversation session
      async initializeVoiceSession() {
        try {
          // Use session data already created on server
          this.conversationSession = this.openaiSession;
          console.log('üéØ Voice session initialized:', this.conversationSession.session_id);
          console.log('üè¢ Company context:', this.conversationSession.company_name);
          console.log('üë§ Stakeholder:', this.conversationSession.stakeholder_name);
          
          // Get WebRTC configuration with ephemeral token
          await this.getWebRTCConfig();
          
        } catch (error) {
          console.error('Error initializing voice session:', error);
          this.addMessage('System', 'Voice session initialization failed. Please refresh the page and try again.', false);
        }
      },

      async getWebRTCConfig() {
        try {
          const response = await fetch(`/api/voice/${this.stakeholderToken}/realtime_config?session_id=${this.conversationSession.session_id}`, {
            method: 'GET',
            headers: {
              'Content-Type': 'application/json'
            }
          });
          
          const data = await response.json();
          if (data.success) {
            this.webrtcConfig = data.config;
            console.log('üîß WebRTC config loaded with ephemeral token');
            console.log('‚è∞ Token expires at:', this.webrtcConfig.session_expires_at);
          } else {
            throw new Error(data.error || 'Failed to get WebRTC config');
          }
        } catch (error) {
          console.error('Error getting WebRTC config:', error);
          throw error;
        }
      },

      // Enhanced microphone toggle with proper WebRTC
      async toggleVoiceConversation() {
        if (!this.isConnected) {
          await this.startVoiceConversation();
        } else {
          await this.stopVoiceConversation();
        }
      },

      async startVoiceConversation() {
        try {
          console.log('üé§ Starting WebRTC voice conversation...');
          
          if (!this.webrtcConfig) {
            await this.getWebRTCConfig();
          }
          
          // Check if ephemeral token is still valid
          const expiresAt = new Date(this.webrtcConfig.session_expires_at);
          if (expiresAt < new Date()) {
            console.log('üîÑ Token expired, refreshing...');
            await this.getWebRTCConfig();
          }
          
          // Create RTCPeerConnection
          this.peerConnection = new RTCPeerConnection({
            iceServers: [{ urls: 'stun:stun.l.google.com:19302' }]
          });
          
          // Set up audio element for AI responses
          this.audioElement = document.createElement('audio');
          this.audioElement.autoplay = true;
          
          // Handle remote audio stream from AI
          this.peerConnection.ontrack = (event) => {
            console.log('üîä Received AI audio stream');
            this.audioElement.srcObject = event.streams[0];
          };
          
          // Request microphone permission and add local audio track
          this.mediaStream = await navigator.mediaDevices.getUserMedia({ 
            audio: {
              sampleRate: 24000,
              channelCount: 1,
              echoCancellation: true,
              noiseSuppression: true,
              autoGainControl: true
            } 
          });
          
          // Add local audio track to peer connection
          this.mediaStream.getTracks().forEach(track => {
            this.peerConnection.addTrack(track, this.mediaStream);
          });
          
          // Create data channel for sending/receiving events
          this.dataChannel = this.peerConnection.createDataChannel('oai-events');
          
          // Handle incoming events from OpenAI
          this.dataChannel.addEventListener('message', (event) => {
            this.handleOpenAIEvent(JSON.parse(event.data));
          });
          
          this.dataChannel.addEventListener('open', () => {
            console.log('‚úÖ Data channel opened - ready for conversation!');
            
            // Send session configuration
            this.sendOpenAIEvent({
              type: 'session.update',
              session: {
                modalities: ['text', 'audio'],
                instructions: this.webrtcConfig.instructions,
                voice: this.webrtcConfig.voice,
                input_audio_format: 'pcm16',
                output_audio_format: 'pcm16',
                input_audio_transcription: { model: 'whisper-1' },
                turn_detection: {
                  type: 'server_vad',
                  threshold: 0.5,
                  prefix_padding_ms: 300,
                  silence_duration_ms: 500
                },
                temperature: 0.7
              }
            });
          });
          
          // Create SDP offer
          const offer = await this.peerConnection.createOffer();
          await this.peerConnection.setLocalDescription(offer);
          
          // Send SDP offer to OpenAI Realtime API
          const response = await fetch(`${this.webrtcConfig.api_endpoint}?model=${this.webrtcConfig.model}`, {
            method: 'POST',
            headers: {
              'Authorization': `Bearer ${this.webrtcConfig.ephemeral_token}`,
              'Content-Type': 'application/sdp'
            },
            body: offer.sdp
          });
          
          if (!response.ok) {
            throw new Error(`SDP exchange failed: ${response.status} ${response.statusText}`);
          }
          
          // Get SDP answer and set as remote description
          const answerSdp = await response.text();
          const answer = {
            type: 'answer',
            sdp: answerSdp
          };
          
          await this.peerConnection.setRemoteDescription(answer);
          
          // Update UI
          this.isConnected = true;
          this.microphoneButton.classList.add('recording-pulse');
          this.microphoneButton.style.backgroundColor = '#ef4444'; // Red when recording
          
          console.log('‚úÖ WebRTC voice conversation started successfully!');
          
        } catch (error) {
          console.error('Error starting WebRTC voice conversation:', error);
          this.addMessage('System', `Failed to start voice conversation: ${error.message}. Please check microphone permissions and try again.`, false);
        }
      },

      sendOpenAIEvent(event) {
        if (this.dataChannel && this.dataChannel.readyState === 'open') {
          this.dataChannel.send(JSON.stringify(event));
        } else {
          console.warn('Data channel not ready for sending events');
        }
      },

      handleOpenAIEvent(event) {
        console.log('üì® OpenAI event:', event.type);
        
        switch (event.type) {
          case 'session.created':
            console.log('‚úÖ Session created');
            break;
            
          case 'conversation.item.input_audio_transcription.completed':
            // User speech transcribed
            if (event.transcript) {
              this.addMessage('You', event.transcript, true);
            }
            break;
            
          case 'response.audio_transcript.delta':
            // AI speech being generated (real-time)
            this.handleAIResponseDelta(event.delta);
            break;
            
          case 'response.audio_transcript.done':
            // AI response complete
            if (event.transcript) {
              this.addMessage('Assistant', event.transcript, false);
            }
            break;
            
          case 'response.done':
            console.log('üéØ AI response completed');
            break;
            
          case 'error':
            console.error('OpenAI error:', event.error);
            this.addMessage('System', `Error: ${event.error.message}`, false);
            break;
            
          default:
            console.log('üìã Unhandled event type:', event.type);
        }
      },

      handleAIResponseDelta(delta) {
        // Handle real-time AI response text (could show typing indicator)
        if (delta) {
          console.log('üî§ AI speaking:', delta);
        }
      },

      async stopVoiceConversation() {
        console.log('üõë Stopping WebRTC voice conversation...');
        
        // Close data channel
        if (this.dataChannel) {
          this.dataChannel.close();
          this.dataChannel = null;
        }
        
        // Close peer connection
        if (this.peerConnection) {
          this.peerConnection.close();
          this.peerConnection = null;
        }
        
        // Stop media stream
        if (this.mediaStream) {
          this.mediaStream.getTracks().forEach(track => track.stop());
          this.mediaStream = null;
        }
        
        // Remove audio element
        if (this.audioElement) {
          this.audioElement.pause();
          this.audioElement.srcObject = null;
          this.audioElement = null;
        }
        
        // Update UI
        this.isConnected = false;
        this.microphoneButton.classList.remove('recording-pulse');
        this.microphoneButton.style.backgroundColor = '';
        
        console.log('‚úÖ WebRTC voice conversation stopped');
      },

      // End voice session
      async endVoiceSession() {
        await this.stopVoiceConversation();
        
        try {
          const response = await fetch(`/api/voice/${this.stakeholderToken}/end`, {
            method: 'POST',
            headers: {
              'Content-Type': 'application/json',
              'X-CSRF-Token': document.querySelector('meta[name="csrf-token"]')?.getAttribute('content')
            },
            body: JSON.stringify({
              session_id: this.conversationSession.session_id
            })
          });
          
          const data = await response.json();
          if (data.success) {
            console.log('üèÅ Voice session ended');
            if (data.final_message) {
              this.addMessage('Assistant', data.final_message, false);
            }
          }
        } catch (error) {
          console.error('Error ending voice session:', error);
        }
      },

      // Send text message (for testing/fallback)
      async sendTextMessage(message) {
        if (!this.conversationSession) {
          console.error('No active conversation session');
          return;
        }
        
        try {
          // Add user message to transcript
          this.addMessage('You', message, true);
          
          const response = await fetch(`/api/voice/${this.stakeholderToken}/message`, {
            method: 'POST',
            headers: {
              'Content-Type': 'application/json',
              'X-CSRF-Token': document.querySelector('meta[name="csrf-token"]')?.getAttribute('content')
            },
            body: JSON.stringify({
              message: message,
              session_id: this.conversationSession.session_id
            })
          });
          
          const data = await response.json();
          if (data.success) {
            console.log('üìù Text message processed');
          }
        } catch (error) {
          console.error('Error sending message:', error);
        }
      }
    };

    // Initialize the voice assessment interface
    VoiceAssessment.init();
    
    // Make it globally available for testing
    window.VoiceAssessment = VoiceAssessment;
  });
</script>

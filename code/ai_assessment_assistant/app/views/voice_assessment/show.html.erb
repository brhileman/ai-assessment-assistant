<!-- Header with Logo -->
<header class="fixed top-0 left-0 right-0 z-40 w-full py-4 flex justify-center">
  <%= image_tag "Blue-Horizontal-LPL-Logo.svg", 
      alt: "LaunchPad Lab",
      class: "h-6 filter brightness-0 invert" %>
</header>

<!-- Gradient Background -->
<div class="min-h-screen bg-gradient-to-br from-blue-900 via-purple-900 to-indigo-900 flex items-center justify-center p-6 pt-24">
  
  <!-- Main Assessment Card -->
  <div class="backdrop-blur-md bg-white/10 rounded-3xl border border-white/20 shadow-2xl p-8 max-w-md w-full">
    
    <!-- Header -->
    <div class="text-center mb-8">
      <h1 class="text-2xl font-bold text-white mb-2">AI Assessment</h1>
      <h2 class="text-lg text-blue-200 mb-1"><%= @company.name %></h2>
      <p class="text-sm text-gray-400" id="welcomeTime"><%= Time.current.strftime("%l:%M %p") %></p>
    </div>
    
    <!-- Wave Visualizer -->
    <div class="mb-8">
      <div class="flex items-end justify-center space-x-1 h-16">
        <% 10.times do |i| %>
          <div class="wave-bar bg-gradient-to-t from-cyan-500 to-blue-400 rounded-full transition-all duration-300" 
               style="width: 4px; height: <%= rand(8..35) %>px;"></div>
        <% end %>
      </div>
    </div>
    
    <!-- Transcript Area -->
    <div class="mb-8">
      <h3 class="text-white text-sm font-semibold mb-3 flex items-center">
        <svg class="w-4 h-4 mr-2 text-cyan-400" fill="currentColor" viewBox="0 0 24 24">
          <path d="M12 3v18l4-4h5a2 2 0 002-2V5a2 2 0 00-2-2H5a2 2 0 00-2 2v10a2 2 0 002 2h5l4 4z"/>
        </svg>
        Conversation
      </h3>
      
      <div id="transcriptContainer" class="bg-slate-900/50 rounded-2xl p-4 h-48 overflow-y-auto border border-slate-700/50">
        <div id="transcriptMessages" class="space-y-4">
          <!-- Messages will appear here -->
        </div>
        
        <!-- Typing Indicator -->
        <div id="typingIndicator" class="hidden flex items-center space-x-2 text-gray-400 text-sm mt-4">
          <div class="flex space-x-1">
            <div class="w-2 h-2 bg-cyan-400 rounded-full animate-bounce"></div>
            <div class="w-2 h-2 bg-cyan-400 rounded-full animate-bounce" style="animation-delay: 0.1s"></div>
            <div class="w-2 h-2 bg-cyan-400 rounded-full animate-bounce" style="animation-delay: 0.2s"></div>
          </div>
          <span>AI is responding...</span>
        </div>
      </div>
    </div>
    
    <!-- Control Bar -->
    <div class="flex items-center justify-center">
      
      <!-- Finish Assessment Button -->
      <button id="finishButton" class="px-6 py-3 bg-gradient-to-r from-red-500 to-red-600 hover:from-red-600 hover:to-red-700 text-white font-semibold rounded-xl shadow-lg border-2 border-red-400/30 hover:border-red-300/50 transition-all duration-300 transform hover:-translate-y-1 flex items-center space-x-2">
        <svg class="w-5 h-5" fill="none" stroke="currentColor" viewBox="0 0 24 24">
          <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 13l4 4L19 7"></path>
        </svg>
        <span>Finish Assessment</span>
      </button>
      
    </div>
    
    <!-- Helper Text -->
    <div class="text-center mt-6">
      <p class="text-gray-400 text-xs" id="statusText">
        Starting your AI readiness conversation...
      </p>
    </div>
    
  </div>
  
</div>

<!-- Modern Confirmation Modal -->
<div id="confirmModal" class="fixed inset-0 z-50 hidden" aria-labelledby="modal-title" role="dialog" aria-modal="true">
  <div class="flex items-center justify-center min-h-screen p-4">
    <div class="fixed inset-0 bg-black/80 backdrop-blur-sm transition-opacity" aria-hidden="true"></div>
    
    <div class="relative backdrop-blur-md bg-white/10 rounded-2xl border border-white/20 shadow-2xl p-6 max-w-sm w-full">
      <div class="text-center">
        <div class="mx-auto mb-4 w-12 h-12 bg-gradient-to-r from-orange-400 to-red-500 rounded-full flex items-center justify-center">
          <svg class="w-6 h-6 text-white" fill="none" stroke="currentColor" viewBox="0 0 24 24">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 9v2m0 4h.01m-6.938 4h13.856c1.54 0 2.502-1.667 1.732-2.5L13.732 4c-.77-.833-1.732-.833-2.5 0L4.268 15.5c-.77.833.192 2.5 1.732 2.5z"></path>
          </svg>
        </div>
        
        <h3 class="text-lg font-semibold text-white mb-2" id="modal-title">
          Finish Assessment?
        </h3>
        <p class="text-gray-300 text-sm mb-6">
          Are you sure you want to complete your assessment? This will save your responses and end the session.
        </p>
        
        <%= form_with url: complete_voice_assessment_path(@stakeholder.invitation_token), 
            method: :patch, 
            class: "space-y-3",
            data: { turbo: false } do |form| %>
          <%= form.hidden_field :final_transcript, id: "finalTranscriptField" %>
          <%= form.submit "Yes, Finish Assessment", 
              class: "w-full px-4 py-3 bg-gradient-to-r from-cyan-500 to-blue-600 text-white font-medium rounded-xl hover:from-cyan-600 hover:to-blue-700 transition-all duration-200" %>
        <% end %>
        
        <button type="button" id="cancelButton" class="w-full px-4 py-3 bg-white/10 text-gray-200 font-medium rounded-xl hover:bg-white/20 transition-all duration-200 border border-white/20">
          Continue Assessment
        </button>
      </div>
    </div>
  </div>
</div>

<!-- Voice Assessment JavaScript -->
<script>
  document.addEventListener('DOMContentLoaded', function() {
    // Initialize voice assessment interface with proper WebRTC
    const VoiceAssessment = {
      init() {
        this.setupElements();
        this.setupEventListeners();
        this.initializeWaveVisualizer();
        this.setWelcomeTime();
        this.initializeVoiceSession();
      },

      setupElements() {
        this.finishButton = document.getElementById('finishButton');
        this.modal = document.getElementById('confirmModal');
        this.cancelButton = document.getElementById('cancelButton');
        this.statusText = document.getElementById('statusText');
        this.transcriptContainer = document.getElementById('transcriptContainer');
        this.transcriptMessages = document.getElementById('transcriptMessages');
        this.typingIndicator = document.getElementById('typingIndicator');
        this.finalTranscriptField = document.getElementById('finalTranscriptField');
        
        this.startTime = new Date();
        this.isConnected = false;
        this.transcriptData = [];
        this.conversationSession = null;
        
        // WebRTC components
        this.peerConnection = null;
        this.dataChannel = null;
        this.audioElement = null;
        this.mediaStream = null;
        this.webrtcConfig = null;
        
        this.stakeholderToken = '<%= @stakeholder.invitation_token %>';
        this.openaiSession = <%= raw @openai_session.to_json %>;
      },

      setupEventListeners() {
        this.finishButton.addEventListener('click', () => this.showConfirmModal());
        this.cancelButton.addEventListener('click', () => this.hideConfirmModal());
        
        // Close modal on backdrop click
        this.modal.addEventListener('click', (e) => {
          if (e.target === this.modal) this.hideConfirmModal();
        });
      },

      async showConfirmModal() {
        // End voice session
        if (this.peerConnection) {
          await this.endVoiceSession();
        }
        
        // Save current transcript to hidden field
        this.finalTranscriptField.value = this.getTranscriptText();
        this.modal.classList.remove('hidden');
      },

      hideConfirmModal() {
        this.modal.classList.add('hidden');
      },

      addMessage(speaker, content, isUser = false) {
        const messageDiv = document.createElement('div');
        messageDiv.className = 'flex space-x-3';
        
        const avatarClass = isUser 
          ? 'w-8 h-8 bg-slate-600 rounded-full flex items-center justify-center'
          : 'w-8 h-8 bg-gradient-to-r from-lpl-blue to-lpl-cyan rounded-full flex items-center justify-center';
        
        const speakerLabel = isUser ? 'You' : 'Assistant';
        const speakerColor = isUser ? 'text-slate-300' : 'text-lpl-cyan';
        
        messageDiv.innerHTML = `
          <div class="flex-shrink-0">
            <div class="${avatarClass}">
              <span class="text-white text-xs font-semibold">${isUser ? 'You' : 'AI'}</span>
            </div>
          </div>
          <div class="flex-1">
            <p class="${speakerColor} text-sm font-medium mb-1">${speakerLabel}</p>
            <p class="text-slate-200 text-sm leading-relaxed">${content}</p>
            <span class="text-slate-500 text-xs mt-1 block">${this.getCurrentTime()}</span>
          </div>
        `;
        
        this.transcriptMessages.appendChild(messageDiv);
        this.scrollToBottom();
        
        // Save to transcript data and backend
        const transcriptEntry = {
          speaker: speakerLabel,
          content: content,
          timestamp: new Date().toISOString(),
          isUser: isUser
        };
        
        this.transcriptData.push(transcriptEntry);
        this.updateBackendTranscript(transcriptEntry);
      },

      async updateBackendTranscript(transcriptEntry) {
        if (!this.conversationSession) return;
        
        try {
          await fetch(`/api/voice/${this.stakeholderToken}/transcript`, {
            method: 'POST',
            headers: {
              'Content-Type': 'application/json',
              'X-CSRF-Token': document.querySelector('meta[name="csrf-token"]')?.getAttribute('content')
            },
            body: JSON.stringify({
              session_id: this.conversationSession.session_id,
              type: transcriptEntry.isUser ? 'user_speech' : 'ai_speech',
              content: transcriptEntry.content,
              speaker: transcriptEntry.isUser ? 'user' : 'assistant'
            })
          });
        } catch (error) {
          console.error('Error updating transcript:', error);
        }
      },

      scrollToBottom() {
        this.transcriptContainer.scrollTop = this.transcriptContainer.scrollHeight;
      },

      getCurrentTime() {
        return new Date().toLocaleTimeString([], { hour: '2-digit', minute: '2-digit' });
      },

      setWelcomeTime() {
        // Welcome time is now set by Rails template - no JavaScript needed
      },

      getTranscriptText() {
        return this.transcriptData.map(msg => 
          `[${msg.timestamp}] ${msg.speaker}: ${msg.content}`
        ).join('\n');
      },

      initializeWaveVisualizer() {
        const waveBars = document.querySelectorAll('.wave-bar');
        
        const randomizeWaves = () => {
          waveBars.forEach((bar, index) => {
            setTimeout(() => {
              const height = Math.random() * 35 + 8;
              bar.style.height = height + 'px';
            }, index * 50);
          });
        };
        
        // Update waves every 1.5 seconds for dynamic effect
        this.waveInterval = setInterval(randomizeWaves, 1500);
      },

      updateWaveVisualizerWithAudio(audioLevel) {
        const waveBars = document.querySelectorAll('.wave-bar');
        waveBars.forEach((bar, index) => {
          const intensity = audioLevel * (1 - Math.abs(index - 5) / 5);
          const height = Math.max(8, intensity * 40);
          bar.style.height = height + 'px';
        });
      },

      // Initialize voice conversation session
      async initializeVoiceSession() {
        try {
          // Use session data already created on server
          this.conversationSession = this.openaiSession;
          console.log('üéØ Voice session initialized:', this.conversationSession.session_id);
          console.log('üè¢ Company context:', this.conversationSession.company_name);
          console.log('üë§ Stakeholder:', this.conversationSession.stakeholder_name);
          
          // Get WebRTC configuration with ephemeral token
          await this.getWebRTCConfig();
          
          // Auto-start the voice conversation
          await this.startVoiceConversation();
          
        } catch (error) {
          console.error('Error initializing voice session:', error);
          this.updateConnectionStatus('error', 'Connection failed. Please refresh and try again.');
          this.addMessage('System', 'Voice session initialization failed. Please refresh the page and try again.', false);
        }
      },

      updateConnectionStatus(state, message) {
        // Update status text only since visual indicators were removed
        switch (state) {
          case 'connecting':
            this.statusText.textContent = message || 'Connecting to AI assistant...';
            break;
          case 'connected':
            this.statusText.textContent = message || 'Connected - Conversation in progress';
            break;
          case 'error':
            this.statusText.textContent = message || 'Connection error';
            this.statusText.className = 'text-red-400 text-xs';
            break;
        }
      },

      async getWebRTCConfig() {
        try {
          const response = await fetch(`/api/voice/${this.stakeholderToken}/realtime_config?session_id=${this.conversationSession.session_id}`, {
            method: 'GET',
            headers: {
              'Content-Type': 'application/json'
            }
          });
          
          const data = await response.json();
          if (data.success) {
            this.webrtcConfig = data.config;
            console.log('üîß WebRTC config loaded with ephemeral token');
            console.log('‚è∞ Token expires at:', this.webrtcConfig.session_expires_at);
          } else {
            throw new Error(data.error || 'Failed to get WebRTC config');
          }
        } catch (error) {
          console.error('Error getting WebRTC config:', error);
          throw error;
        }
      },

      async startVoiceConversation() {
        try {
          console.log('üé§ Starting WebRTC voice conversation...');
          this.updateConnectionStatus('connecting', 'Requesting microphone access...');
          
          if (!this.webrtcConfig) {
            await this.getWebRTCConfig();
          }
          
          // Check if ephemeral token is still valid
          const expiresAt = new Date(this.webrtcConfig.session_expires_at);
          if (expiresAt < new Date()) {
            console.log('üîÑ Token expired, refreshing...');
            await this.getWebRTCConfig();
          }
          
          // Create RTCPeerConnection
          this.peerConnection = new RTCPeerConnection({
            iceServers: [{ urls: 'stun:stun.l.google.com:19302' }]
          });
          
          // Set up audio element for AI responses
          this.audioElement = document.createElement('audio');
          this.audioElement.autoplay = true;
          
          // Handle remote audio stream from AI
          this.peerConnection.ontrack = (event) => {
            console.log('üîä Received AI audio stream');
            this.audioElement.srcObject = event.streams[0];
          };
          
          // Request microphone permission and add local audio track
          this.mediaStream = await navigator.mediaDevices.getUserMedia({ 
            audio: {
              sampleRate: 24000,
              channelCount: 1,
              echoCancellation: true,
              noiseSuppression: true,
              autoGainControl: true
            } 
          });
          
          this.updateConnectionStatus('connecting', 'Connecting to AI assistant...');
          
          // Add local audio track to peer connection
          this.mediaStream.getTracks().forEach(track => {
            this.peerConnection.addTrack(track, this.mediaStream);
          });
          
          // Create data channel for sending/receiving events
          this.dataChannel = this.peerConnection.createDataChannel('oai-events');
          
          // Handle incoming events from OpenAI
          this.dataChannel.addEventListener('message', (event) => {
            this.handleOpenAIEvent(JSON.parse(event.data));
          });
          
          this.dataChannel.addEventListener('open', () => {
            console.log('‚úÖ Data channel opened - ready for conversation!');
            
            // Send session configuration
            this.sendOpenAIEvent({
              type: 'session.update',
              session: {
                modalities: ['text', 'audio'],
                instructions: this.webrtcConfig.instructions,
                voice: this.webrtcConfig.voice,
                input_audio_format: 'pcm16',
                output_audio_format: 'pcm16',
                input_audio_transcription: { model: 'whisper-1' },
                turn_detection: {
                  type: 'server_vad',
                  threshold: 0.5,
                  prefix_padding_ms: 300,
                  silence_duration_ms: 500
                },
                temperature: 0.7
              }
            });
            
            // Trigger AI to start the conversation immediately
            setTimeout(() => {
              this.sendOpenAIEvent({
                type: 'response.create',
                response: {
                  modalities: ['text', 'audio'],
                  instructions: `You are LaunchPad Lab's AI early discovery assistant conducting an AI readiness assessment. Begin with: "Hi ${this.conversationSession.stakeholder_name}! I'm LaunchPad Lab's trained AI discovery assistant." Then briefly explain this is a 3-5 minute conversation to understand ${this.conversationSession.company_name}'s current state of technology and AI interests. Ask about their main responsibilities at ${this.conversationSession.company_name} to start the discovery process.`
                }
              });
            }, 1000); // Small delay to ensure session is fully configured
          });
          
          // Create SDP offer
          const offer = await this.peerConnection.createOffer();
          await this.peerConnection.setLocalDescription(offer);
          
          // Send SDP offer to OpenAI Realtime API
          const response = await fetch(`${this.webrtcConfig.api_endpoint}?model=${this.webrtcConfig.model}`, {
            method: 'POST',
            headers: {
              'Authorization': `Bearer ${this.webrtcConfig.ephemeral_token}`,
              'Content-Type': 'application/sdp'
            },
            body: offer.sdp
          });
          
          if (!response.ok) {
            throw new Error(`SDP exchange failed: ${response.status} ${response.statusText}`);
          }
          
          // Get SDP answer and set as remote description
          const answerSdp = await response.text();
          const answer = {
            type: 'answer',
            sdp: answerSdp
          };
          
          await this.peerConnection.setRemoteDescription(answer);
          
          // Update UI to connected state
          this.isConnected = true;
          this.updateConnectionStatus('connected');
          
          console.log('‚úÖ WebRTC voice conversation started successfully!');
          
        } catch (error) {
          console.error('Error starting WebRTC voice conversation:', error);
          this.updateConnectionStatus('error', `Failed to start conversation: ${error.message}`);
          this.addMessage('System', `Failed to start voice conversation: ${error.message}. Please check microphone permissions and try again.`, false);
        }
      },

      sendOpenAIEvent(event) {
        if (this.dataChannel && this.dataChannel.readyState === 'open') {
          this.dataChannel.send(JSON.stringify(event));
        } else {
          console.warn('Data channel not ready for sending events');
        }
      },

      handleOpenAIEvent(event) {
        console.log('üì® OpenAI event:', event.type);
        
        switch (event.type) {
          case 'session.created':
            console.log('‚úÖ Session created');
            break;
            
          case 'conversation.item.input_audio_transcription.completed':
            // User speech transcribed
            if (event.transcript) {
              this.addMessage('You', event.transcript, true);
            }
            break;
            
          case 'response.audio_transcript.delta':
            // AI speech being generated (real-time)
            this.handleAIResponseDelta(event.delta);
            break;
            
          case 'response.audio_transcript.done':
            // AI response complete
            if (event.transcript) {
              this.addMessage('Assistant', event.transcript, false);
            }
            break;
            
          case 'response.done':
            console.log('üéØ AI response completed');
            break;
            
          case 'error':
            console.error('OpenAI error:', event.error);
            this.addMessage('System', `Error: ${event.error.message}`, false);
            break;
            
          default:
            console.log('üìã Unhandled event type:', event.type);
        }
      },

      handleAIResponseDelta(delta) {
        // Handle real-time AI response text (could show typing indicator)
        if (delta) {
          console.log('üî§ AI speaking:', delta);
        }
      },

      async stopVoiceConversation() {
        console.log('üõë Stopping WebRTC voice conversation...');
        
        // Close data channel
        if (this.dataChannel) {
          this.dataChannel.close();
          this.dataChannel = null;
        }
        
        // Close peer connection
        if (this.peerConnection) {
          this.peerConnection.close();
          this.peerConnection = null;
        }
        
        // Stop media stream
        if (this.mediaStream) {
          this.mediaStream.getTracks().forEach(track => track.stop());
          this.mediaStream = null;
        }
        
        // Remove audio element
        if (this.audioElement) {
          this.audioElement.pause();
          this.audioElement.srcObject = null;
          this.audioElement = null;
        }
        
        // Update UI to disconnected state
        this.isConnected = false;
        this.updateConnectionStatus('connecting', 'Conversation ended');
        
        console.log('‚úÖ WebRTC voice conversation stopped');
      },

      // End voice session
      async endVoiceSession() {
        await this.stopVoiceConversation();
        
        try {
          const response = await fetch(`/api/voice/${this.stakeholderToken}/end`, {
            method: 'POST',
            headers: {
              'Content-Type': 'application/json',
              'X-CSRF-Token': document.querySelector('meta[name="csrf-token"]')?.getAttribute('content')
            },
            body: JSON.stringify({
              session_id: this.conversationSession.session_id
            })
          });
          
          const data = await response.json();
          if (data.success) {
            console.log('üèÅ Voice session ended');
            if (data.final_message) {
              this.addMessage('Assistant', data.final_message, false);
            }
          }
        } catch (error) {
          console.error('Error ending voice session:', error);
        }
      },

      // Send text message (for testing/fallback)
      async sendTextMessage(message) {
        if (!this.conversationSession) {
          console.error('No active conversation session');
          return;
        }
        
        try {
          // Add user message to transcript
          this.addMessage('You', message, true);
          
          const response = await fetch(`/api/voice/${this.stakeholderToken}/message`, {
            method: 'POST',
            headers: {
              'Content-Type': 'application/json',
              'X-CSRF-Token': document.querySelector('meta[name="csrf-token"]')?.getAttribute('content')
            },
            body: JSON.stringify({
              message: message,
              session_id: this.conversationSession.session_id
            })
          });
          
          const data = await response.json();
          if (data.success) {
            console.log('üìù Text message processed');
          }
        } catch (error) {
          console.error('Error sending message:', error);
        }
      }
    };

    // Initialize the voice assessment interface
    VoiceAssessment.init();
    
    // Make it globally available for testing
    window.VoiceAssessment = VoiceAssessment;
  });
</script>
